{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654842f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1493b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tree = ET.parse('../data/raw/all2.xml')\n",
    "root = tree.getroot()\n",
    "#print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55b1257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   '4EM General Model',\n",
      "    'Goal Model',\n",
      "    'Product-Service-Model',\n",
      "    'Technical Components and Requirements Model',\n",
      "    'Business Process Model',\n",
      "    'Goal & Business Rule Model',\n",
      "    'Actors and Resources Model',\n",
      "    'Concepts Model',\n",
      "    'Business Rule & Process Model']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "xml_models = root[0]\n",
    "model_classes = []\n",
    "\n",
    "for m in xml_models:\n",
    "    model_classes.append(m.attrib[\"modeltype\"])\n",
    "    \n",
    "model_classes = list(set(model_classes))\n",
    "\n",
    "num_model_classes = len(model_classes)\n",
    "\n",
    "def get_model_class(model):\n",
    "    return model_classes.index(model.attrib[\"modeltype\"])\n",
    "\n",
    "\n",
    "#pp.pprint(model_classes)\n",
    "#print(len(model_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df932fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_models = root[0]\n",
    "model_data = []\n",
    "node_classes = []\n",
    "edge_classes = []\n",
    "\n",
    "for m in xml_models:\n",
    "    instances = [el for el in m if el.tag == \"INSTANCE\"]\n",
    "    connectors = [el for el in m if el.tag == \"CONNECTOR\"]\n",
    "    \n",
    "    for instance in instances:\n",
    "        node_class = instance.attrib[\"class\"]\n",
    "        node_classes.append(node_class)\n",
    "    \n",
    "    for connector in connectors:\n",
    "        edge_type = next(filter(lambda attr: attr.get(\"name\") == \"Type\", connector.findall(\"ATTRIBUTE\"))).text\n",
    "        if(edge_type is None):\n",
    "            edge_type = \"none\"\n",
    "        edge_classes.append(edge_type.lower())\n",
    "        \n",
    "\n",
    "node_classes = list(set(node_classes))\n",
    "edge_classes = list(set(edge_classes))\n",
    "\n",
    "num_node_classes = len(node_classes)\n",
    "num_edge_classes = len(edge_classes)\n",
    "\n",
    "\n",
    "#pp.pprint(node_classes)\n",
    "#pp.pprint(num_node_classes)\n",
    "\n",
    "#pp.pprint(edge_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91229027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseModelDatasetNCPerModeltype(Dataset):\n",
    "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.filename = filename\n",
    "        self.num_classes = num_node_classes\n",
    "        super(EnterpriseModelDatasetNCPerModeltype, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        return \"unimplemented.pt\"\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in range(len(xml_models))]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in range(len(xml_models))]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        xml_models = root[0]\n",
    "        index = 0\n",
    "        \n",
    "        \n",
    "        for m in xml_models:\n",
    "            model = {}\n",
    "            nodes = []\n",
    "            edges = []\n",
    "            adjacency_list = []\n",
    "            y = []\n",
    "\n",
    "            nodes_data = []\n",
    "            edges_data = []\n",
    "            \n",
    "\n",
    "            instances = [el for el in m if el.tag == \"INSTANCE\"]\n",
    "            connectors = [el for el in m if el.tag == \"CONNECTOR\"]\n",
    "\n",
    "        \n",
    "            for instance in instances:\n",
    "                node = {}\n",
    "                node_class = instance.attrib[\"class\"]\n",
    "                node_name = instance.attrib[\"name\"]\n",
    "                node[\"class\"] = node_class\n",
    "                node[\"name\"] = node_name\n",
    "                nodes_data.append(node)\n",
    "            \n",
    "                # Dataset relevant\n",
    "                nodes.append([node_classes.index(node_class)])\n",
    "                \n",
    "                #node_y = list(0 for i in range(0,num_node_classes))\n",
    "                #node_y[node_classes.index(node_class)] = 1\n",
    "                #y.append(node_y)\n",
    "                \n",
    "                node_y = node_classes.index(node_class)\n",
    "                y.append(torch.tensor(node_y, dtype=torch.int64))\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "            for connector in connectors:\n",
    "                edge = {}\n",
    "                edge_type = next(filter(lambda attr: attr.get(\"name\") == \"Type\", connector.findall(\"ATTRIBUTE\"))).text\n",
    "                if(edge_type is None):\n",
    "                    edge_type = \"none\"\n",
    "                else:\n",
    "                    edge_type = edge_type.lower()\n",
    "                edge[\"type\"] = edge_type\n",
    "\n",
    "                connector_from = connector.find(\"FROM\").get(\"instance\")\n",
    "                connector_to = connector.find(\"TO\").get(\"instance\")\n",
    "                edge[\"from\"] = connector_from\n",
    "                edge[\"to\"] = connector_to\n",
    "                edges_data.append(edge)\n",
    "\n",
    "                from_index = [node_data[\"name\"] for node_data in nodes_data].index(connector_from)\n",
    "                to_index = [node_data[\"name\"] for node_data in nodes_data].index(connector_to)\n",
    "                \n",
    "                # Dataset relevant\n",
    "                adjacency_list.append([from_index, to_index])\n",
    "                edges.append([edge_classes.index(edge_type)])\n",
    "                \n",
    "                \n",
    "        \n",
    "            model[\"nodes\"] = torch.tensor(nodes, dtype=torch.float)\n",
    "            model[\"edges\"] = torch.tensor(edges, dtype=torch.float)\n",
    "            model[\"adjacency\"] = torch.tensor(adjacency_list, dtype=torch.int64)\n",
    "            #model[\"y\"] = torch.tensor(y, dtype=torch.float)\n",
    "            model[\"y\"] = torch.tensor(y, dtype=torch.int64)\n",
    "            \n",
    "\n",
    "            model[\"nodes_data\"] = nodes_data\n",
    "            model[\"edges_data\"] = edges_data\n",
    "            \n",
    "            proba_0 = 0.75\n",
    "            train_mask = np.random.choice([True, False], size=len(model[\"nodes\"]), p=[proba_0, 1-proba_0])\n",
    "            test_mask = np.asarray([not val for val in train_mask])\n",
    "            \n",
    "            \n",
    "         # Create data object\n",
    "            data = Data(x=model[\"nodes\"], \n",
    "                        edge_index= model[\"adjacency\"].t().contiguous(),\n",
    "                        edge_attr=model[\"edges\"],\n",
    "                        y=model[\"y\"],\n",
    "                        train_mask=torch.tensor(train_mask),\n",
    "                        test_mask=torch.tensor(test_mask),\n",
    "                        modeltype=get_model_class(m)\n",
    "                        )\n",
    "            \n",
    "            \n",
    "            if self.test:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{index}.pt'))\n",
    "            else:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "            \n",
    "            index += 1\n",
    "\n",
    "        \n",
    "    def len(self):\n",
    "        return len(xml_models)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_{idx}.pt'))   \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c0c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset_nc_per_modeltype = EnterpriseModelDatasetNCPerModeltype(root=\"../data/nc_data_per_model\", filename=\"../raw/all2.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dee8db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_model = {\"info\": {}, \"data\": {}}\n",
    "for i in range(0, num_model_classes):\n",
    "    per_model[\"data\"][model_classes[i]] = []\n",
    "    per_model[\"info\"][model_classes[i]] = 0\n",
    "for data in dataset_nc_per_modeltype:\n",
    "    per_model[\"data\"][model_classes[data.modeltype]].append(data)\n",
    "    per_model[\"info\"][model_classes[data.modeltype]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea2be0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72f686b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET LOADED\n",
      "EnterpriseModelDatasetNCPerModeltype(110)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET LOADED: USE per_model variable !!!\")\n",
    "print(dataset_nc_per_modeltype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f19f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc4364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487f3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg_env]",
   "language": "python",
   "name": "conda-env-pyg_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
